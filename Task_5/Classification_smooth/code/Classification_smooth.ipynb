{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import Preprocess\n",
    "import platform\n",
    "\n",
    "file_path = os.path.abspath(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "if platform.system().lower() == 'windows':\n",
    "    print(\"windows\")\n",
    "    dataset_X = np.load(file_path + \"\\Classification_smooth\\data\\dataset_X.npy\")\n",
    "    dataset_Y = np.load(file_path + \"\\Classification_smooth\\data\\dataset_Y.npy\")\n",
    "elif platform.system().lower() == 'linux':\n",
    "    print(\"linux\")    \n",
    "    dataset_X = np.load(file_path + \"/Classification_smooth/data/dataset_X.npy\")\n",
    "    dataset_Y = np.load(file_path + \"/Classification_smooth/data/dataset_Y.npy\")\n",
    "\n",
    "\n",
    "\n",
    "data_X = []\n",
    "data_Y = []\n",
    "\n",
    "num = 24\n",
    "\n",
    "for i in range(0, len(dataset_X)-num, 24):\n",
    "    data_X.append(dataset_X[i:i+num])\n",
    "    data_Y.append(dataset_Y[i:i+num])\n",
    "    \n",
    "data_X = np.array(data_X)\n",
    "data_Y = np.array(data_Y)\n",
    "\n",
    "train_X = data_X[:1200]\n",
    "train_Y = data_Y[:1200]\n",
    "test_X = data_X[1200:]\n",
    "test_Y = data_Y[1200:]\n",
    "\n",
    "Preprocess.save_as_npy(train_X, train_Y, test_X, test_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_path = os.path.abspath(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "\n",
    "if platform.system().lower() == 'windows':\n",
    "    print(\"windows\")\n",
    "    train_X = np.load(file_path + \"\\Classification_smooth\\data\\\\train_X.npy\")\n",
    "    train_Y = np.load(file_path + \"\\Classification_smooth\\data\\\\train_Y.npy\")\n",
    "    test_X = np.load(file_path + \"\\Classification_smooth\\data\\\\test_X.npy\")\n",
    "    test_Y = np.load(file_path + \"\\Classification_smooth\\data\\\\test_Y.npy\")\n",
    "elif platform.system().lower() == 'linux':\n",
    "    print(\"linux\")    \n",
    "train_X = np.load(file_path + \"/Classification_smooth/data/train_X.npy\")\n",
    "train_Y = np.load(file_path + \"/Classification_smooth/data/train_Y.npy\")\n",
    "test_X = np.load(file_path + \"/Classification_smooth/data/test_X.npy\")\n",
    "test_Y = np.load(file_path + \"/Classification_smooth/data/test_Y.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.customed_dataset import customed_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "file_path = os.path.abspath(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "# dataset_X = np.load(file_path + \"\\Task_2\\data\\\\dataset_X.npy\")\n",
    "# dataset_Y = np.load(file_path + \"\\Task_2\\data\\\\dataset_Y.npy\")\n",
    "\n",
    "# dataset = customed_dataset(dataset_X, dataset_Y)\n",
    "\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataset = customed_dataset(train_X, train_Y)\n",
    "test_dataset = customed_dataset(test_X, test_Y)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "validation_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import lstm\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "model = lstm.lstm_model().cuda()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2000\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = lstm.train_one_epoch(model, epoch_number, training_loader, optimizer, loss_fn)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.float().cuda()\n",
    "        vlabels = vlabels.float().cuda()\n",
    "        voutputs = model(vinputs)\n",
    "        voutputs = torch.squeeze(voutputs)\n",
    "        #print(voutputs.size())\n",
    "        #print(vlabels.size())\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/LSTM_model'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import gru\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "model = gru.gru_model().cuda()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2000\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = gru.train_one_epoch(model, epoch_number, training_loader, optimizer, loss_fn)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    h_0 = torch.zeros(1, 4, 32).cuda()\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.float().cuda()\n",
    "        vlabels = vlabels.float().cuda()\n",
    "        if vinputs.size()[0] != 4:\n",
    "            continue    \n",
    "        voutputs = model(vinputs)\n",
    "        voutputs = torch.squeeze(voutputs)\n",
    "        #print(voutputs.size())\n",
    "        #print(vlabels.size())\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/GRU_model'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models import lstm\n",
    "from models import gru\n",
    "from models import transformer\n",
    "\n",
    "\n",
    "\n",
    "### lstm\n",
    "model_lstm = lstm.lstm_model()\n",
    "model_lstm.load_state_dict(torch.load('models/LSTM_model'))\n",
    "model_lstm.eval()\n",
    "\n",
    "### gru\n",
    "model_gru = gru.gru_model()\n",
    "model_gru.load_state_dict(torch.load('models/GRU_model'))\n",
    "model_gru.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lstm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lstm_predictions = []\n",
    "lstm_targets = []\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "    lstm_predictions.append(model_lstm.forward(data.float()).detach().numpy().tolist())\n",
    "    lstm_targets.append(target.numpy().tolist())\n",
    "lstm_predictions = torch.squeeze(torch.tensor(lstm_predictions)).reshape(-1).numpy()\n",
    "lstm_targets = torch.squeeze(torch.tensor(lstm_targets)).reshape(-1).numpy()\n",
    "\n",
    "for i in range(0, len(lstm_predictions)):\n",
    "    if lstm_predictions[i] <= 35:\n",
    "        lstm_predictions[i] = 0\n",
    "    elif lstm_predictions[i] <= 150:\n",
    "        lstm_predictions[i] = 1\n",
    "    else:\n",
    "        lstm_predictions[i] = 2\n",
    "    if lstm_targets[i] <= 35:\n",
    "        lstm_targets[i] = 0\n",
    "    elif lstm_targets[i] <= 150:\n",
    "        lstm_targets[i] = 1\n",
    "    else:\n",
    "        lstm_targets[i] = 2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "print(accuracy_score(lstm_targets, lstm_predictions))\n",
    "print(f1_score(lstm_targets, lstm_predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gru test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model_gru = model_gru.cuda()\n",
    "\n",
    "gru_predictions = []\n",
    "gru_targets = []\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "    if len(data) != 4:\n",
    "        continue\n",
    "    gru_predictions.append(model_gru.forward(data.float().cuda()).cpu().detach().numpy().tolist())\n",
    "    gru_targets.append(target.numpy().tolist())\n",
    "gru_predictions = torch.squeeze(torch.tensor(gru_predictions)).reshape(-1).numpy()\n",
    "gru_targets = torch.squeeze(torch.tensor(gru_targets)).reshape(-1).numpy()\n",
    "\n",
    "for i in range(0, len(gru_predictions)):\n",
    "    if gru_predictions[i] <= 35:\n",
    "        gru_predictions[i] = 0\n",
    "    elif gru_predictions[i] <= 150:\n",
    "        gru_predictions[i] = 1\n",
    "    else:\n",
    "        gru_predictions[i] = 2\n",
    "    if gru_targets[i] <= 35:\n",
    "        gru_targets[i] = 0\n",
    "    elif gru_targets[i] <= 150:\n",
    "        gru_targets[i] = 1\n",
    "    else:\n",
    "        gru_targets[i] = 2\n",
    "\n",
    "print(accuracy_score(gru_targets, gru_predictions))\n",
    "print(f1_score(gru_targets, gru_predictions,average='macro'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
